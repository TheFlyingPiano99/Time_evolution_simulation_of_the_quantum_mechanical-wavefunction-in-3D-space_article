\section{Our implementation}
\label{sec:our_implementation}

Using the Fourier method we created a Python application simulating the time development of the quantum mechanical wave function.
We use ray tracing to visualize resulting volumetric of probability density.
The visualization requires the sampling of 3D data set on a discretized grid.
This makes it impossible to fully reconstruct the wave function that we simulated using only a finite resolution to begin with.
In order to fight sampling artifacts we deploy a state of the art triquadratic reconstruction filter recently proposed by Balázs Csébfalvi \cite{csebfalvi2023}.

We choose Python \cite{van1995python} as our programming language because there is a waste amount of helpful tools implemented to use with it that specifically aim towards leveraging the difficulty of writing mathematical and physics related simulation.
The majority of these tools are written in a hardware friendly languages such as Fortran, C or C++.
However they come with an easy-to-use \acrfull{api} that can be accessed from high level programming languages.
In these modern languages such as Python one works on a higher abstraction level usually not dealing with memory management ore pointer arithmetic.
Out implementation heavily depends on the Numpy library \cite{harris2020array}.
NumPy is a library that can be utilized to work efficiently with large arrays and tensors performing computationally intensive operations.
A similar library is SciPy \cite{2020SciPy-NMeth} also used in our program.
For 3D visualization we choose to use VisPy \cite{vispy}.
This library provides a nice basic set of features to handle virtual camera and create scenes but also enables us to go deeper and write our own \acrshort{gpu} shader code.
We made use of this to modify the default volume visualization code to fit our needs.

The Fourier method described in section \ref{sec:used_method} opens up the possibility to implement the simulation on the \acrshort{gpu}.
The \acrfull{cuda} toolkit is often used for parallel computational tasks implemented on the \acrshort{gpu} \cite{cuda2008}.
It comes with a powerful \acrshort{gpu} based \acrshort{fft} implementation.
To use \acrshort{cuda} with Python we selected the CuPy wrapping library \cite{cupy_learningsys2017} that provides abstraction over \acrshort{cuda}.
For other purposes we use multiple other libraries such as Imageio, Matplotlib, toml, Pillow, Keyboard, Colorama, Numba, Tqdm, PyQt5, PyOpenGL.
The sources for these libraries can be found on the internet.
We have listed the required versions in a requirements.txt file in the projects GitHub repository.

In its current state our application has only a console interface and it saves the resulting images and videos into files.
The reason that we so far haven't prioritized the development of a graphical user interface is that we think that terminal interfaces can still have their benefits even in 2023.
An application that only requires a terminal to function is simpler thus more effort can be made to improve the core functionality.
The audience of this software are scientists and engineers in the first place.
This is especially true in the early stage of development that we are currently in.
It already has however a snapshot system that makes it possible to interrupt a longer simulation and later resume from the exact state where it was previously halted.
This turned out to be a very convenient feature although since we use \acrshort{gpu} acceleration the simulation times are generally much shorter.
Our design philosophy dictated to communicate as much information about the simulated wave function towards the user as possible.
This direction can be thought of as controversial since too much data can obfuscate the important details especially in a plain console print.
We try to battle this effect by using colorful prints and by saving the text into a log file so that the parameters can be found even after the simulation has finished.

To specify the parameters of a simulation we use a configuration file in \acrfull{toml} format \cite{toml}.
The flexibility and universality of this data-format made it an overall good choice.
We can use a single \acrshort{toml} file to set the resolution and dimensions of the simulated volume, specify the parameters of the wave packet, add an arbitrary number of various potential barriers.
I this file we have parameters to configure the 3D visualization and others.

As we have mentioned earlier the output of the program gets saved in files.
The program creates images and videos and text files.
The images can be thought of as higher resolution snapshots from the videos but we also create images that are not corresponding to any of the videos.
Currently we generate five types of different images.
We call these canvas dwell time, canvas probability, per axis probability density, probability density 3D and finally, probability evolution.
In each of these categories a sequence of images is being generated for each run of the simulation.
The interval at which the state of the simulation is captured can be specified in the above described configuration file.
Canvas dwell time is the integral of probability density over time in a specified plane intersecting the simulated volume.
Canvas probability is the probability density in a specified plane intersecting the volume.
Both dwell time and probability density can be used to visualize interference patterns after a wave packet passes through some kind of a potential barrier such as a double-slit or an optical grid.
Per axis probability density gives a projected view on the probability density.
This not a real image but rather a plot of probability density as the function of spatial coordinates separately for each axis.
To obtain this plot we integrated the probability density so that for a specific $i_j = x$ discretized spatial coordinate we sum the probability density where $i_j = x$ and the other two coordinates run across their domain.
We also overlap the plot of the potential barriers on the same graph.
This helps to understand the changes in propagation of the wave packet.
This type of plot can provide useful information for most of the simulation cases, but it is especially useful when we want to simulate the behavior of three one dimensional particles in a three dimensional configuration space.
In this case the projected probability density along each axis represent the wave packet of a different 1D particle.
It is possible to initialize a potential that model the collision between these three particles.
This creates the effect that the particles interact.
In the configuration file there is a possibility to set whether a potential barrier should appear in the visualization.
By disabling the visualization of the interaction potential we can get rid of any hardly conceptualizable elements of the resulting potential and maybe focus on a wall or a harmonic oscillator instead.
Probability density 3D is the most self explanatory output of the system.
Here we take the probability density as a 3D volumetric data set and visualize it using ray tracing.
Ray tracing is the method where we conceptually shoot rays of light from the virtual camera through the visualized volume.
For each pixel of the image there is exactly one ray.
We progress along the ray in discrete steps and at each step we sample the data set.
The data read from the volume has usually a single intensity value.
We interpret the data using a transfer function that determines the color and opacity that should be associated with the intensity.
The color and opacity is then combined along the ray resulting in the final color and opacity value for the pixel of the image.
The operator that recursively combines the color and opacity values is called under operator. For recursive calculation of the $A_n$ opacity in $n$th step along the ray it can be written in the form of
\begin{equation}
	\label{eq:under_op}
	A_n = A_{n-1} + \alpha_n(1 - A_{n-1})
\end{equation}
where $A_{n-1}$ is the opacity calculated at the previous step and $\alpha_n$ is the local opacity at the $n$th step.
A similar equation could be written for red, green and blue (RGB) color channels.
If we would only use the colors obtained by simply stepping through the sample points the look of the probability density would be rather dull.
We also utilize approximated gradient vectors.
One can calculate a these vectors by taking the central difference using six additional samples around the currently sampled point.
We take samples along each of three axes so that one of them is before and one is after the current central sample along each axis, hence the six additional samples.
If we choose a small enough $\delta$ step size than we can successfully approximate the spatial derivatives along each axis using the following differential
\begin{equation}
	\label{eq:central_differential}
	dp = \frac{p(x + \delta) - p(x - \delta)}{2\delta}
\end{equation}
The gradient vector points towards in the direction in which the probability density increases the fastest.
If we make the analogy that a solid body objects has also got gradient vectors right on the edge of it's surface so that these vectors point towards the inside of the object perpendicular to the surface
we can make the connection between gradient vectors and surface normals.
Normal vectors of a surface are unit length vectors pointing towards the outside world from the surface and are also perpendicular to the surface.
Normal vectors can be expressed as negated and normalized gradients.
Normal vectors are often used in various surface shading models, since they give a good description how surfaces reflect light.
Even tho our probability density has a much less clearly defined gradient this technique can still be used to obtain an approximated normal vector.
If this vector is sufficiently defined (the magnitude of the gradient is not too close to zero) than we can use simple shading model such as the Blinn-Phong reflection model \cite{Blinn1977} to enhance our visuals.
We even use the length information of the gradient to fade between the shaded and unshaded look.
We would like to mention that in our previous works we have experimented with more physically plausible shading techniques such as applying the Cook-Torrance \acrfull{brdf} \cite{CookTorrance1982}.
This however has not yielded good results for volumetric visualization since surface imperfections are calculated into the model thus the generally fussy nature of volumetric datasets result in an overall dark look.
The simple Blinn-Phong model however is cheap to compute, and provides sufficiently strong specular highlights while it's still easy to parameterize.
For most of our simulations we use a total simulated volume of $512\times 512 \times 512$ data points.
We leave the outer half of this volume to accommodate the draining potential described in the last part of section \ref{sec:used_method}.
In addition to the basic central difference technique we utilized Balázs Csébfalvi's reconstruction filter \cite{csebfalvi2023}.
This method makes use of the already evaluated samples required for the central difference and at the cost of a few additional samples it performs a polynom fitting thus improves the quality of the original sample point evaluated in the center.












